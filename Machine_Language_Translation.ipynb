{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Language Translation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjrdy/Machine-Language-Translation/blob/master/Machine_Language_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faDuukaFW4dO",
        "colab_type": "text"
      },
      "source": [
        "# Creating a translation system from a parallel corpus is a resource-intensive process (processor, memory, GPU).\n",
        "\n",
        "***First Go to Runtime and  change the runtime type to GPU.***\n",
        "\n",
        "\n",
        "<br>\n",
        " Copyright:  Vijay YERUVA \n",
        "<br>\n",
        " Email: vijay.yeruva@epita.fr\n",
        "<br>\n",
        " UID: #20527\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TqaR59cW92r",
        "colab_type": "text"
      },
      "source": [
        "# Git Clone\n",
        "First Git clone the OpenNMT source"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4x8zn_UvqUb",
        "colab_type": "code",
        "outputId": "9717fec3-e94a-4fe9-b482-b12ba0c4dbed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcpRdR8TBefS",
        "colab_type": "code",
        "outputId": "6d092df1-0d70-4f4c-f674-ee87f28c0580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "!ls gdrive/My\\ Drive/model_bckp/run/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "assets\n",
            "checkpoint\n",
            "eval\n",
            "events.out.tfevents.1563743941.b5886273c130\n",
            "events.out.tfevents.1563744101.b5886273c130\n",
            "export\n",
            "graph.pbtxt\n",
            "model.ckpt-12000.data-00000-of-00002\n",
            "model.ckpt-12000.data-00001-of-00002\n",
            "model.ckpt-12000.index\n",
            "model.ckpt-12000.meta\n",
            "model.ckpt-13000.data-00000-of-00002\n",
            "model.ckpt-13000.data-00001-of-00002\n",
            "model.ckpt-13000.index\n",
            "model.ckpt-13000.meta\n",
            "model.ckpt-14000.data-00000-of-00002\n",
            "model.ckpt-14000.data-00001-of-00002\n",
            "model.ckpt-14000.index\n",
            "model.ckpt-14000.meta\n",
            "model.ckpt-15000.data-00000-of-00002\n",
            "model.ckpt-15000.data-00001-of-00002\n",
            "model.ckpt-15000.index\n",
            "model.ckpt-15000.meta\n",
            "model.ckpt-16000.data-00000-of-00002\n",
            "model.ckpt-16000.data-00001-of-00002\n",
            "model.ckpt-16000.index\n",
            "model.ckpt-16000.meta\n",
            "model_description.py\n",
            "projector_config.pbtxt\n",
            "seq2seq_decoder_w_embs.txt\n",
            "seq2seq_encoder_w_embs.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPzx4ck-TKPE",
        "colab_type": "code",
        "outputId": "acc7eb3c-8294-4694-91f3-4fe8b86074df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/OpenNMT/OpenNMT-tf.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OpenNMT-tf'...\n",
            "remote: Enumerating objects: 1284, done.\u001b[K\n",
            "remote: Counting objects: 100% (1284/1284), done.\u001b[K\n",
            "remote: Compressing objects: 100% (485/485), done.\u001b[K\n",
            "remote: Total 16561 (delta 857), reused 1171 (delta 791), pack-reused 15277\u001b[K\n",
            "Receiving objects: 100% (16561/16561), 14.73 MiB | 25.43 MiB/s, done.\n",
            "Resolving deltas: 100% (13274/13274), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW8KRFlDZj5h",
        "colab_type": "code",
        "outputId": "4d2ab549-ee5a-4875-ff4e-8a9217792aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tOpenNMT-tf  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67O0_4c6m7pd",
        "colab_type": "code",
        "outputId": "50ef02c7-08df-4f5d-af7c-8aa2ab098d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "pip install OpenNMT-tf[tensorflow_gpu]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting OpenNMT-tf[tensorflow_gpu]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/e9/eef6ceccc0fe17cb283453b626161b1a40967cee0eb09e864b6280d8e7db/OpenNMT_tf-1.24.0-py2.py3-none-any.whl (149kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 3.4MB/s \n",
            "\u001b[?25hCollecting rouge==0.3.1 (from OpenNMT-tf[tensorflow_gpu])\n",
            "  Downloading https://files.pythonhosted.org/packages/8f/89/af359c22e1d858e0299d4cc9219f36b504817c9797acad23081247867845/rouge-0.3.1-py3-none-any.whl\n",
            "Collecting pyyaml>=5.1 (from OpenNMT-tf[tensorflow_gpu])\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 44.7MB/s \n",
            "\u001b[?25hCollecting pyonmttok<2,>=1.11.0; platform_system == \"Linux\" (from OpenNMT-tf[tensorflow_gpu])\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/9e/7d58d9e2605a526950b6ce09335ae3af939c133e11515ba95207de2cde78/pyonmttok-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 48.3MB/s \n",
            "\u001b[?25hCollecting sacrebleu==1.*; python_version >= \"3.0\" (from OpenNMT-tf[tensorflow_gpu])\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/d6/258a1e63463b4731a387f0872dca759c330bf4845cc0464f2c65028674b6/sacrebleu-1.3.7.tar.gz\n",
            "Collecting tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\" (from OpenNMT-tf[tensorflow_gpu])\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 83kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu==1.*; python_version >= \"3.0\"->OpenNMT-tf[tensorflow_gpu]) (3.7.4)\n",
            "Collecting portalocker (from sacrebleu==1.*; python_version >= \"3.0\"->OpenNMT-tf[tensorflow_gpu])\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/4a/940f5184098d054804605e73258d57707b1df087fc532111593e6110be13/portalocker-1.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (3.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.16.4)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu<2,>=1.4.0; extra == \"tensorflow_gpu\"->OpenNMT-tf[tensorflow_gpu]) (0.15.5)\n",
            "Building wheels for collected packages: pyyaml, sacrebleu\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
            "  Building wheel for sacrebleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/26/75/6598d35c0498433bbfb118f37bf85da0eb5dc672836d025553\n",
            "Successfully built pyyaml sacrebleu\n",
            "Installing collected packages: rouge, pyyaml, pyonmttok, portalocker, sacrebleu, tensorflow-gpu, OpenNMT-tf\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed OpenNMT-tf-1.24.0 portalocker-1.5.0 pyonmttok-1.14.0 pyyaml-5.1.1 rouge-0.3.1 sacrebleu-1.3.7 tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXV4phskXAKf",
        "colab_type": "text"
      },
      "source": [
        "# Please install  OpenNMT-tensorflow use by pip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMAlbjUfTl3B",
        "colab_type": "code",
        "outputId": "c16d243d-a6d1-48ed-93e9-41c2a91a3b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.14"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==1.14 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.33.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.1.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.16.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCA2nZBLXH1r",
        "colab_type": "text"
      },
      "source": [
        "# Theory explanation\n",
        "\n",
        "**Machine translation is a field of natural language processing, meaning that computers translate one language into another.**\n",
        "\n",
        "Rule based, and statistical based, and recently we are using Deep Learning-based machine translation.\n",
        "\n",
        "Learn how to build a real machine translation system and how the system pipeline is structured. Most of these courses can be applied to basic natural language processing problems as well as machine translation.\n",
        "\n",
        "**Step**\n",
        "\n",
        "\n",
        "\n",
        "**1.   Data Collection**\n",
        "\n",
        "Parallel corpus is collected from various sources. It is possible to collect news texts, drama / movie subtitles, Wikipedia, etc., as well as data sets for evaluation of translation systems disclosed by WMT, a machine translation competition, and use them in translation systems.\n",
        "\n",
        "\n",
        "**2.   Cleaning**\n",
        "\n",
        "The collected data must be refined. The refinement process includes sorting sentences by corpus in both languages, and eliminating noise such as special characters.\n",
        "\n",
        "\n",
        "**3. Subword Tokenization**\n",
        "\n",
        "Refine spacing using the POS tagger or segmenter for each language. English may have refinement issues in upper / lower case.\n",
        "After the spacing is refined, use Byte Pair Encoding (BPE) using public tools such as Subword or WordPiece. This allows you to perform additional segments and construct a vocabulary list. At this time, the segmented models learned for the BPE segment should be kept for future use.\n",
        "\n",
        "\n",
        "**4. Train**\n",
        "\n",
        "Train the seq2seq model using prepared datasets. Depending on the amount, you can train with a single GPU, or use multiple GPUs in parallel to reduce training time.\n",
        "\n",
        "\n",
        "**5. Translate**\n",
        "\n",
        "Now that the model has been created, you can start translating.\n",
        "\n",
        "\n",
        "**6. Detokenization**\n",
        "\n",
        "Even after the translation process is finished, it is still in a segment, so it is different from the actual sentence structure used by real people. Thus, when you perform a detoxification process, it is returned in the form of the actual sentence.\n",
        "\n",
        "\n",
        "**7. Evaluating**\n",
        "\n",
        "Quantitative evaluation is performed on the sentence thus obtained. BLEU is a quantitative evaluation method for machine translation. You can see which model is superior by comparing it to the BLEU score you are comparing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwwT9bcnXKvy",
        "colab_type": "text"
      },
      "source": [
        "# Data Collection\n",
        "\n",
        "Let's Collect en-de Parallel Corpus form amazon S3\n",
        "In your Colab Files A directory called toy-ende would have been created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z4f2_nfTwQf",
        "colab_type": "code",
        "outputId": "34252e84-4166-4b0c-e9d8-a71b52a09f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://cservan.github.io/website/data/tp4_files.tgz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-22 20:07:11--  https://cservan.github.io/website/data/tp4_files.tgz\n",
            "Resolving cservan.github.io (cservan.github.io)... 185.199.110.153, 185.199.111.153, 185.199.109.153, ...\n",
            "Connecting to cservan.github.io (cservan.github.io)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4517825 (4.3M) [application/octet-stream]\n",
            "Saving to: ‘tp4_files.tgz’\n",
            "\n",
            "tp4_files.tgz       100%[===================>]   4.31M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-07-22 20:07:12 (49.0 MB/s) - ‘tp4_files.tgz’ saved [4517825/4517825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWhXzI8SUfw3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f17d0fd-f2e1-4954-9797-933fa9b6b186"
      },
      "source": [
        "\n",
        "!rm data-l.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'data-l.tar.gz': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiAxCAEVHLET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xf  tp4_files.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4mXvfeOHR_I",
        "colab_type": "code",
        "outputId": "63583d5f-7be0-4c1e-9eb3-4cf04e49434f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: target 'data_translation/ep7_tst.fr' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldpFAMGu1DIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat data_translation/EMEA_train.en >> train.en\n",
        "!cat data_translation/ep7_train.en >> train.en\n",
        "!cat data_translation/EMEA_train.fr >> train.fr\n",
        "!cat data_translation/ep7_train.fr >> train.fr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bTgx3_NXVXi",
        "colab_type": "text"
      },
      "source": [
        "# Subword Tokenization\n",
        "\n",
        "We use Byte Pair Encoding for Subword Tokenization\n",
        "\n",
        "https://www.aclweb.org/anthology/P16-1162\n",
        "\n",
        "i => input<br>\n",
        "o ==> Output(*.code)<br>\n",
        "s ==> Symbol (Usually use 32000)<br>\n",
        "\n",
        "learn_bpe ==> make code<br>\n",
        "apply_bpe ==> apply subwordTokenization<br>\n",
        "\n",
        "src-train, src-val,test ==> Need to apply src.code<br>\n",
        "tgt-train,tgt-val ==> Need to apply tgt.code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj0O2mTGcDq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python OpenNMT-tf/third_party/learn_bpe.py -i train.fr -o fr.code -s 16000\n",
        "!python OpenNMT-tf/third_party/learn_bpe.py -i train.en -o en.code -s 16000\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c fr.code -i train.fr -o train.fr.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c en.code -i train.en -o train.en.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c fr.code -i data_translation/EMEA_dev.fr -o EMEA_dev.fr.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c en.code -i data_translation/EMEA_dev.en -o EMEA_dev.en.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c fr.code -i data_translation/ep7_dev.fr -o ep7_dev.fr.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c en.code -i data_translation/ep7_dev.en -o ep7_dev.en.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c fr.code -i data_translation/EMEA_tst.fr -o EMEA_tst.fr.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c en.code -i data_translation/EMEA_tst.en -o EMEA_tst.en.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c fr.code -i data_translation/ep7_tst.fr -o ep7_tst.fr.bpe\n",
        "!python OpenNMT-tf/third_party/apply_bpe.py -c en.code -i data_translation/ep7_tst.en -o ep7_tst.en.bpe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X71vIyGaZZqP",
        "colab_type": "text"
      },
      "source": [
        "# Build Vocab\n",
        "\n",
        "We will be working with some example data in toy-ende/ folder.\n",
        "​\n",
        "The data consists of parallel source (src) and target (tgt) data containing one sentence per line with tokens separated by a space:\n",
        "​\n",
        "1. src-train.txt\n",
        "​\n",
        "2. tgt-train.txt\n",
        "​\n",
        "3. src-val.txt\n",
        "​\n",
        "4. tgt-val.txt\n",
        "​\n",
        "​\n",
        "\n",
        "Train data and validataion data are required for machine translation training.\n",
        "​\n",
        "Validation files are required and used to evaluate the convergence of the training. It usually contains no more than 5000 sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbHK1VRdIIS2",
        "colab_type": "code",
        "outputId": "4f43cb01-2626-4ea6-9be4-06327742c7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!onmt-build-vocab --size 50000 --save_vocab vocab.fr train.fr.bpe\n",
        "!onmt-build-vocab --size 50000 --save_vocab vocab.en train.en.bpe"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 20:10:09.476788 139918424053632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/decoders/rnn_decoder.py:435: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "W0722 20:10:10.589747 139918424053632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/adafactor.py:32: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0722 20:10:10.590256 139918424053632 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/multistep_adam.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 20:10:15.957321 140192497944448 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/decoders/rnn_decoder.py:435: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "W0722 20:10:16.785666 140192497944448 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/adafactor.py:32: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0722 20:10:16.786179 140192497944448 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/multistep_adam.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGabgf96UiA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "d1c8ba4d-c629-4ada-faad-56d69d084c9e"
      },
      "source": [
        "!onmt-build-vocab --size 50000 --save_vocab toy-ende/src-vocab.txt toy-ende/src-train-bpe.txt"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 20:10:26.298670 140307859224448 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/decoders/rnn_decoder.py:435: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "W0722 20:10:27.140803 140307859224448 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/adafactor.py:32: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0722 20:10:27.141323 140307859224448 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/multistep_adam.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt-build-vocab\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/opennmt/bin/build_vocab.py\", line 52, in main\n",
            "    vocab.add_from_text(data_file, tokenizer=tokenizer)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/opennmt/utils/vocab.py\", line 68, in add_from_text\n",
            "    for line in text:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 220, in __next__\n",
            "    return self.next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 214, in next\n",
            "    retval = self.readline()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 178, in readline\n",
            "    self._preread_check()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py\", line 84, in _preread_check\n",
            "    compat.as_bytes(self.__name), 1024 * 512)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: toy-ende/src-train-bpe.txt; No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAsE2ANOUz0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!onmt-build-vocab --size 50000 --save_vocab toy-ende/tgt-vocab.txt toy-ende/tgt-train-bpe.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3t0Q2FpZ7Kx",
        "colab_type": "text"
      },
      "source": [
        "# Let's Make data.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHP7ldm9VIU6",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "model_dir: toy-ende/run/\n",
        "\n",
        "data:\n",
        "  train_features_file: toy-ende/src-train.txt\n",
        "  train_labels_file: toy-ende/tgt-train.txt\n",
        "  eval_features_file: toy-ende/src-val.txt\n",
        "  eval_labels_file: toy-ende/tgt-val.txt\n",
        "  source_words_vocabulary: toy-ende/src-vocab.txt\n",
        "  target_words_vocabulary: toy-ende/tgt-vocab.txt\n",
        "\n",
        "train:\n",
        "  save_checkpoints_steps: 1000\n",
        "\n",
        "  eval:\n",
        "    eval_delay: 3600  # Every 1 hour\n",
        "    external_evaluators: BLEU\n",
        "infer:\n",
        "    batch_size: 32\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAXcmCGnbD41",
        "colab_type": "text"
      },
      "source": [
        "**Create a data.yml file on your computer and upload it to Google Colab.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jVmFBfULWZQ",
        "colab_type": "code",
        "outputId": "05de4ef7-012d-41f3-f171-29c43efa9d12",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-662495e1-7cff-4003-b087-bd5287707a62\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-662495e1-7cff-4003-b087-bd5287707a62\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.yml to data.yml\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data.yml': b\"model_dir: 'gdrive/My Drive/model/run/'\\n\\ndata:\\n  train_features_file: train.fr.bpe\\n  train_labels_file: train.en.bpe\\n  eval_features_file: ep7_dev.fr.bpe\\n  eval_labels_file: ep7_dev.en.bpe\\n  source_words_vocabulary: vocab.fr\\n  target_words_vocabulary: vocab.en\\n\\ntrain:\\n  batch_size: 1024\\n  save_checkpoints_steps: 1000\\n  maximum_features_length: 50\\n  maximum_labels_length: 50\\n\\n  eval:\\n    eval_delay: 3600  # Every 1 hour\\n    external_evaluators: BLEU\\ninfer:\\n    batch_size: 32\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K194z1dIbc4t",
        "colab_type": "text"
      },
      "source": [
        "# **Train the data(Basic)**\n",
        "\n",
        "This command will start the training and evaluation loop of a small RNN-based sequence to sequence model.\n",
        "\n",
        "If you want to use GPU , try add  below command (example use 1 GPU)\n",
        ">  --num_gpus 1\n",
        "\n",
        "Let's Check Available GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoxVx_NYbskx",
        "colab_type": "code",
        "outputId": "635e8d10-7dd7-4a9f-da38-961572f7c3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 21 19:35:33 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKBsDEH1b8Mk",
        "colab_type": "text"
      },
      "source": [
        "**Let's Train**\n",
        "\n",
        "Model is locate in toy-ende/run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTMj2tvv1lYF",
        "colab_type": "code",
        "outputId": "bfa4211c-f0ad-4856-cafb-e0c016c9dda1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "config = tf.ConfigProto(device_count = {'GPU': 1})\n",
        "tf.Session(config=config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.client.session.Session at 0x7fad109bae10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HszqVQZd5N6E",
        "colab_type": "code",
        "outputId": "b5179777-9f5e-4425-a821-d27395cbbe68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "print(sess.list_devices())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 19:05:27.706065 140174334187392 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 14129114277990025488), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 4892493333467479829), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2641036094187648022), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 11326691738, 4360997152523710507)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKqxq1WM50o_",
        "colab_type": "code",
        "outputId": "59fe8427-95e9-471b-c270-d0f46a08dbed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "def get_available_gpus():\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
        "get_available_gpus()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vt93FfN2C36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF__8D8pbz7D",
        "colab_type": "text"
      },
      "source": [
        "# **Train the data(Transformer)**\n",
        "\n",
        "https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf\n",
        "\n",
        "\n",
        "> If you get GPU-related errors, try halving batch_size\n",
        "\n",
        "\n",
        "***Just Change model_type***\n",
        "\n",
        "Available Model ==> http://opennmt.net/OpenNMT-tf/package/opennmt.models.catalog.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9BRLlahcD8I",
        "colab_type": "code",
        "outputId": "1c34764c-9378-43a2-acc7-50d5b9cdaf47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!onmt-main train_and_eval --model_type NMTSmall --auto_config --config data.yml --num_gpus 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0722 20:19:59.463116 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/decoders/rnn_decoder.py:435: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "W0722 20:20:00.304202 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/adafactor.py:32: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0722 20:20:00.304729 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/multistep_adam.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0722 20:20:00.312719 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:111: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0722 20:20:00.312883 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:111: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0722 20:20:00.316754 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:145: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "W0722 20:20:00.531238 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:146: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0722 20:20:00.531412 140047122683776 main.py:146] Creating model directory gdrive/My Drive/model/run/\n",
            "W0722 20:20:00.531568 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:147: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "2019-07-22 20:20:00.535210: W tensorflow/python/util/util.cc:280] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "W0722 20:20:00.546290 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:154: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0722 20:20:00.547191 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:157: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "I0722 20:20:00.634816 140047122683776 runner.py:98] Using parameters:\n",
            "data:\n",
            "  eval_features_file: ep7_dev.fr.bpe\n",
            "  eval_labels_file: ep7_dev.en.bpe\n",
            "  source_words_vocabulary: vocab.fr\n",
            "  target_words_vocabulary: vocab.en\n",
            "  train_features_file: train.fr.bpe\n",
            "  train_labels_file: train.en.bpe\n",
            "eval:\n",
            "  batch_size: 32\n",
            "  eval_delay: 18000\n",
            "  exporters: last\n",
            "infer:\n",
            "  batch_size: 32\n",
            "  bucket_width: 5\n",
            "model_dir: gdrive/My Drive/model/run/\n",
            "params:\n",
            "  beam_width: 4\n",
            "  clip_gradients: 5.0\n",
            "  learning_rate: 0.0002\n",
            "  optimizer: AdamOptimizer\n",
            "  param_init: 0.1\n",
            "score:\n",
            "  batch_size: 64\n",
            "train:\n",
            "  batch_size: 1024\n",
            "  batch_type: examples\n",
            "  bucket_width: 1\n",
            "  eval:\n",
            "    eval_delay: 3600\n",
            "    external_evaluators: BLEU\n",
            "  maximum_features_length: 50\n",
            "  maximum_labels_length: 50\n",
            "  sample_buffer_size: -1\n",
            "  save_checkpoints_steps: 1000\n",
            "  save_summary_steps: 100\n",
            "  train_steps: 500000\n",
            "\n",
            "2019-07-22 20:20:00.637918: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2019-07-22 20:20:00.643731: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-22 20:20:00.762984: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:00.763567: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c8aa00 executing computations on platform CUDA. Devices:\n",
            "2019-07-22 20:20:00.763599: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-07-22 20:20:00.765519: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-22 20:20:00.765698: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c8b100 executing computations on platform Host. Devices:\n",
            "2019-07-22 20:20:00.765728: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-22 20:20:00.765948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:00.766377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-22 20:20:00.766680: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-22 20:20:00.767999: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-22 20:20:00.769177: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-22 20:20:00.769507: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-22 20:20:00.770941: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-22 20:20:00.772029: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-22 20:20:00.775040: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-22 20:20:00.775189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:00.775589: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:00.775915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-22 20:20:00.775973: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-22 20:20:00.776886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-22 20:20:00.776911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-22 20:20:00.776921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-22 20:20:00.777237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:00.777664: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:00.778082: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-22 20:20:00.778144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 14089 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0722 20:20:00.779014 140047122683776 estimator.py:209] Using config: {'_model_dir': 'gdrive/My Drive/model/run/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': gpu_options {\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    layout_optimizer: OFF\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f5eda82bdd8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0722 20:20:00.779562 140047122683776 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "I0722 20:20:00.779768 140047122683776 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "I0722 20:20:00.780009 140047122683776 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 1000 or save_checkpoints_secs None.\n",
            "W0722 20:20:00.786017 140047122683776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "I0722 20:20:01.058441 140047122683776 data.py:292] Training on 60000 examples\n",
            "W0722 20:20:01.063877 140047122683776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/opennmt/tokenizers/tokenizer.py:277: calling string_split (from tensorflow.python.ops.ragged.ragged_string_ops) with delimiter is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "delimiter is deprecated, please use sep instead.\n",
            "W0722 20:20:01.074313 140047122683776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/lookup_ops.py:978: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0722 20:20:01.129178 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/compat.py:64: The name tf.data.get_output_shapes is deprecated. Please use tf.compat.v1.data.get_output_shapes instead.\n",
            "\n",
            "W0722 20:20:01.155077 140047122683776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/estimator.py:129: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0722 20:20:01.160030 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/estimator.py:131: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0722 20:20:01.160194 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/estimator.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "I0722 20:20:01.162281 140047122683776 estimator.py:1145] Calling model_fn.\n",
            "W0722 20:20:01.175204 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/parallel.py:143: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0722 20:20:01.175609 140047122683776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/opennmt/models/model.py:102: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0722 20:20:01.176066 140047122683776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0722 20:20:01.310139 140047122683776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/cell.py:49: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0722 20:20:01.310627 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/cell.py:56: The name tf.nn.rnn_cell.DropoutWrapper is deprecated. Please use tf.compat.v1.nn.rnn_cell.DropoutWrapper instead.\n",
            "\n",
            "W0722 20:20:01.315353 140047122683776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/cell.py:64: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0722 20:20:01.315863 140047122683776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/encoders/rnn_encoder.py:80: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0722 20:20:01.683211 140047122683776 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0722 20:20:02.379155 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/models/sequence_to_sequence.py:191: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0722 20:20:02.885246 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/decoders/decoder.py:63: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "W0722 20:20:03.745955 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/estimator.py:262: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0722 20:20:05.642927 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/inputters/text_inputter.py:44: The name tf.gfile.Copy is deprecated. Please use tf.io.gfile.copy instead.\n",
            "\n",
            "W0722 20:20:05.651440 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/inputters/text_inputter.py:47: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0722 20:20:05.654166 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/inputters/text_inputter.py:75: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0722 20:20:05.675405 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/hooks.py:132: The name tf.train.SecondOrStepTimer is deprecated. Please use tf.estimator.SecondOrStepTimer instead.\n",
            "\n",
            "I0722 20:20:05.675690 140047122683776 estimator.py:1147] Done calling model_fn.\n",
            "I0722 20:20:05.675913 140047122683776 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "I0722 20:20:06.152038 140047122683776 hooks.py:22] Number of trainable parameters: 35059954\n",
            "W0722 20:20:06.152406 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/hooks.py:152: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
            "\n",
            "W0722 20:20:06.162193 140047122683776 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/hooks.py:159: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
            "\n",
            "I0722 20:20:06.324242 140047122683776 monitored_session.py:240] Graph was finalized.\n",
            "2019-07-22 20:20:06.324922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:06.325350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-22 20:20:06.325431: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-22 20:20:06.325454: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-22 20:20:06.325475: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-22 20:20:06.325496: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-22 20:20:06.325520: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-22 20:20:06.325539: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-22 20:20:06.325559: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-22 20:20:06.325663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:06.326053: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:06.326430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-22 20:20:06.326469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-22 20:20:06.326482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-22 20:20:06.326495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-22 20:20:06.326763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:06.327193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-22 20:20:06.327534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14089 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2019-07-22 20:20:07.976174: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "I0722 20:20:07.982423 140047122683776 session_manager.py:500] Running local_init_op.\n",
            "I0722 20:20:08.079453 140047122683776 session_manager.py:502] Done running local_init_op.\n",
            "I0722 20:20:09.688927 140047122683776 basic_session_run_hooks.py:606] Saving checkpoints for 0 into gdrive/My Drive/model/run/model.ckpt.\n",
            "2019-07-22 20:20:12.935971: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "I0722 20:20:17.165724 140047122683776 basic_session_run_hooks.py:262] loss = 115.96694, step = 0\n",
            "I0722 20:21:33.292299 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.31359\n",
            "I0722 20:21:33.293481 140047122683776 basic_session_run_hooks.py:260] loss = 175.3941, step = 100 (76.128 sec)\n",
            "I0722 20:22:46.314671 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.36944\n",
            "I0722 20:22:46.316275 140047122683776 basic_session_run_hooks.py:260] loss = 34.333115, step = 200 (73.023 sec)\n",
            "I0722 20:22:46.316578 140047122683776 hooks.py:184] source_words/sec: 20039\n",
            "I0722 20:22:46.316752 140047122683776 hooks.py:184] target_words/sec: 18910\n",
            "I0722 20:23:51.477491 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.53462\n",
            "I0722 20:23:51.478594 140047122683776 basic_session_run_hooks.py:260] loss = 146.13467, step = 300 (65.162 sec)\n",
            "I0722 20:23:51.478900 140047122683776 hooks.py:184] source_words/sec: 19953\n",
            "I0722 20:23:51.479095 140047122683776 hooks.py:184] target_words/sec: 18879\n",
            "I0722 20:25:05.052410 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.35916\n",
            "I0722 20:25:05.054156 140047122683776 basic_session_run_hooks.py:260] loss = 255.3673, step = 400 (73.576 sec)\n",
            "I0722 20:25:05.054472 140047122683776 hooks.py:184] source_words/sec: 20324\n",
            "I0722 20:25:05.054639 140047122683776 hooks.py:184] target_words/sec: 19083\n",
            "I0722 20:26:17.543664 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.37948\n",
            "I0722 20:26:17.544857 140047122683776 basic_session_run_hooks.py:260] loss = 61.32455, step = 500 (72.491 sec)\n",
            "I0722 20:26:17.545065 140047122683776 hooks.py:184] source_words/sec: 20335\n",
            "I0722 20:26:17.545242 140047122683776 hooks.py:184] target_words/sec: 19136\n",
            "I0722 20:27:32.886646 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.32726\n",
            "I0722 20:27:32.888417 140047122683776 basic_session_run_hooks.py:260] loss = 73.77502, step = 600 (75.344 sec)\n",
            "I0722 20:27:32.888659 140047122683776 hooks.py:184] source_words/sec: 20543\n",
            "I0722 20:27:32.888830 140047122683776 hooks.py:184] target_words/sec: 19403\n",
            "I0722 20:28:38.595682 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.52186\n",
            "I0722 20:28:38.596713 140047122683776 basic_session_run_hooks.py:260] loss = 62.394234, step = 700 (65.708 sec)\n",
            "I0722 20:28:38.596897 140047122683776 hooks.py:184] source_words/sec: 20379\n",
            "I0722 20:28:38.597043 140047122683776 hooks.py:184] target_words/sec: 19284\n",
            "I0722 20:29:44.672073 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.5134\n",
            "I0722 20:29:44.674064 140047122683776 basic_session_run_hooks.py:260] loss = 167.04236, step = 800 (66.077 sec)\n",
            "I0722 20:29:44.674421 140047122683776 hooks.py:184] source_words/sec: 20228\n",
            "I0722 20:29:44.674614 140047122683776 hooks.py:184] target_words/sec: 19075\n",
            "I0722 20:31:01.886487 140047122683776 basic_session_run_hooks.py:692] global_step/sec: 1.29509\n",
            "I0722 20:31:01.887526 140047122683776 basic_session_run_hooks.py:260] loss = 262.00452, step = 900 (77.213 sec)\n",
            "I0722 20:31:01.887727 140047122683776 hooks.py:184] source_words/sec: 20292\n",
            "I0722 20:31:01.887891 140047122683776 hooks.py:184] target_words/sec: 18992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RShN75dccRsl",
        "colab_type": "text"
      },
      "source": [
        "# **Translate**\n",
        "\n",
        "Now that you have your model, you can start translating.\n",
        "\n",
        "\n",
        "\n",
        "Output predictions into pred.txt\n",
        "\n",
        "Translate Using desired model\n",
        "\n",
        "--checkpoint_path run/baseline-enfr/avg/model.ckpt-200000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irAAKzR7WPNa",
        "colab_type": "code",
        "outputId": "07cdd3e1-3d45-4540-a10e-e2e4cb179ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!onmt-main infer --auto_config    --config data.yml      --features_file toy-ende/src-test.txt --predictions_file toy-ende/pred.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 19:30:37.586095 139732698646400 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
            "W0721 19:30:37.626138 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/decoders/rnn_decoder.py:435: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n",
            "\n",
            "W0721 19:30:38.613473 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/adafactor.py:32: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0721 19:30:38.614068 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/optimizers/multistep_adam.py:36: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0721 19:30:38.622879 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:111: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0721 19:30:38.623063 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:111: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0721 19:30:38.627677 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:145: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
            "\n",
            "W0721 19:30:38.627973 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/config.py:112: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0721 19:30:38.628121 139732698646400 config.py:112] Loading model description from run/model_description.py\n",
            "2019-07-21 19:30:38.630081: W tensorflow/python/util/util.cc:280] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "W0721 19:30:38.638397 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:154: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0721 19:30:38.638571 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py:157: The name tf.GPUOptions is deprecated. Please use tf.compat.v1.GPUOptions instead.\n",
            "\n",
            "I0721 19:30:38.729064 139732698646400 runner.py:98] Using parameters:\n",
            "data:\n",
            "  eval_features_file: ep7_dev.fr.bpe\n",
            "  eval_labels_file: ep7_dev.en.bpe\n",
            "  source_words_vocabulary: vocab.fr\n",
            "  target_words_vocabulary: vocab.en\n",
            "  train_features_file: train.fr.bpe\n",
            "  train_labels_file: train.en.bpe\n",
            "eval:\n",
            "  batch_size: 32\n",
            "  eval_delay: 18000\n",
            "  exporters: last\n",
            "infer:\n",
            "  batch_size: 32\n",
            "  bucket_width: 5\n",
            "model_dir: run/\n",
            "params:\n",
            "  average_loss_in_time: true\n",
            "  beam_width: 4\n",
            "  decay_params:\n",
            "    model_dim: 512\n",
            "    warmup_steps: 8000\n",
            "  decay_type: noam_decay_v2\n",
            "  label_smoothing: 0.1\n",
            "  learning_rate: 2.0\n",
            "  optimizer: LazyAdamOptimizer\n",
            "  optimizer_params:\n",
            "    beta1: 0.9\n",
            "    beta2: 0.998\n",
            "score:\n",
            "  batch_size: 64\n",
            "train:\n",
            "  average_last_checkpoints: 8\n",
            "  batch_size: 1024\n",
            "  batch_type: tokens\n",
            "  bucket_width: 1\n",
            "  effective_batch_size: 25000\n",
            "  eval:\n",
            "    eval_delay: 3600\n",
            "    external_evaluators: BLEU\n",
            "  keep_checkpoint_max: 8\n",
            "  maximum_features_length: 50\n",
            "  maximum_labels_length: 50\n",
            "  sample_buffer_size: -1\n",
            "  save_checkpoints_steps: 20\n",
            "  save_summary_steps: 20\n",
            "  train_steps: 300\n",
            "\n",
            "2019-07-21 19:30:38.730874: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2019-07-21 19:30:38.737228: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-21 19:30:38.800387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:38.800950: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13f3dc0 executing computations on platform CUDA. Devices:\n",
            "2019-07-21 19:30:38.801002: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-07-21 19:30:38.803360: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-21 19:30:38.803605: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13f4680 executing computations on platform Host. Devices:\n",
            "2019-07-21 19:30:38.803644: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-21 19:30:38.803910: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:38.804330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-21 19:30:38.804671: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 19:30:38.806375: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-21 19:30:38.807822: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-21 19:30:38.808237: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-21 19:30:38.810280: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-21 19:30:38.811664: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-21 19:30:38.816025: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-21 19:30:38.816173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:38.816629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:38.817015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-21 19:30:38.817100: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 19:30:38.818307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-21 19:30:38.818341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-21 19:30:38.818358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-21 19:30:38.818681: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:38.819167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:38.819536: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-21 19:30:38.819594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/device:GPU:0 with 10742 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "I0721 19:30:38.820750 139732698646400 estimator.py:209] Using config: {'_model_dir': 'run/', '_tf_random_seed': None, '_save_summary_steps': 20, '_save_checkpoints_steps': 20, '_save_checkpoints_secs': None, '_session_config': gpu_options {\n",
            "}\n",
            "allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    layout_optimizer: OFF\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 8, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 20, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f15a59f7c88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "W0721 19:30:38.865710 139732698646400 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/opennmt/tokenizers/tokenizer.py:277: calling string_split (from tensorflow.python.ops.ragged.ragged_string_ops) with delimiter is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "delimiter is deprecated, please use sep instead.\n",
            "W0721 19:30:38.878314 139732698646400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/lookup_ops.py:978: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0721 19:30:38.886209 139732698646400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/data.py:369: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.data.get_output_shapes(dataset)`.\n",
            "W0721 19:30:38.886450 139732698646400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/data.py:371: enumerate_dataset (from tensorflow.python.data.experimental.ops.enumerate_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.enumerate()\n",
            "W0721 19:30:38.914410 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/utils/compat.py:64: The name tf.data.get_output_shapes is deprecated. Please use tf.compat.v1.data.get_output_shapes instead.\n",
            "\n",
            "W0721 19:30:38.936204 139732698646400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/estimator.py:129: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n",
            "W0721 19:30:38.942350 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/estimator.py:131: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0721 19:30:38.942529 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/estimator.py:131: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "I0721 19:30:38.944523 139732698646400 estimator.py:1145] Calling model_fn.\n",
            "W0721 19:30:38.946645 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/models/model.py:85: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0721 19:30:38.946972 139732698646400 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/opennmt/models/transformer.py:136: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0721 19:30:39.009552 139732698646400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/encoders/self_attention_encoder.py:59: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "W0721 19:30:39.119153 139732698646400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/opennmt/layers/transformer.py:136: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv1D` instead.\n",
            "W0721 19:30:40.965158 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/decoders/decoder.py:63: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
            "\n",
            "W0721 19:30:44.605868 139732698646400 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/opennmt/estimator.py:223: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "I0721 19:30:44.606264 139732698646400 estimator.py:1147] Done calling model_fn.\n",
            "I0721 19:30:45.227488 139732698646400 monitored_session.py:240] Graph was finalized.\n",
            "2019-07-21 19:30:45.228365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:45.228941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-21 19:30:45.229026: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 19:30:45.229067: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-21 19:30:45.229144: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-21 19:30:45.229183: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-21 19:30:45.229235: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-21 19:30:45.229278: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-21 19:30:45.229318: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-21 19:30:45.229456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:45.229955: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:45.230372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-21 19:30:45.230423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-21 19:30:45.230445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-21 19:30:45.230463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-21 19:30:45.230762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:45.231223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 19:30:45.231603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10742 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0721 19:30:45.231866 139732698646400 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "I0721 19:30:45.233197 139732698646400 saver.py:1280] Restoring parameters from run/model.ckpt-300\n",
            "2019-07-21 19:30:45.617366: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: Key transformer/decoder/LayerNorm/beta not found in checkpoint\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1356, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1341, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1429, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\n",
            "  (0) Not found: Key transformer/decoder/LayerNorm/beta not found in checkpoint\n",
            "\t [[{{node save/RestoreV2}}]]\n",
            "  (1) Not found: Key transformer/decoder/LayerNorm/beta not found in checkpoint\n",
            "\t [[{{node save/RestoreV2}}]]\n",
            "\t [[save/RestoreV2/_17]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1286, in restore\n",
            "    {self.saver_def.filename_tensor_name: save_path})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 950, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1173, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1350, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1370, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: 2 root error(s) found.\n",
            "  (0) Not found: Key transformer/decoder/LayerNorm/beta not found in checkpoint\n",
            "\t [[node save/RestoreV2 (defined at /lib/python3.6/dist-packages/opennmt/runner.py:423) ]]\n",
            "  (1) Not found: Key transformer/decoder/LayerNorm/beta not found in checkpoint\n",
            "\t [[node save/RestoreV2 (defined at /lib/python3.6/dist-packages/opennmt/runner.py:423) ]]\n",
            "\t [[save/RestoreV2/_17]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "Original stack trace for 'save/RestoreV2':\n",
            "  File \"/bin/onmt-main\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/lib/python3.6/dist-packages/opennmt/bin/main.py\", line 186, in main\n",
            "    log_time=args.log_prediction_time)\n",
            "  File \"/lib/python3.6/dist-packages/opennmt/runner.py\", line 423, in infer\n",
            "    hooks=infer_hooks):\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 635, in predict\n",
            "    hooks=all_hooks) as mon_sess:\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\n",
            "    stop_grace_period_secs=stop_grace_period_secs)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 725, in __init__\n",
            "    self._sess = _RecoverableSession(self._coordinated_creator)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1200, in __init__\n",
            "    _WrappedSession.__init__(self, self._create_session())\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1205, in _create_session\n",
            "    return self._sess_creator.create_session()\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 871, in create_session\n",
            "    self.tf_sess = self._session_creator.create_session()\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 638, in create_session\n",
            "    self._scaffold.finalize()\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 229, in finalize\n",
            "    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 599, in _get_saver_or_default\n",
            "    saver = Saver(sharded=True, allow_empty=True)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n",
            "    self.build()\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\n",
            "    self._build(self._filename, build_save=True, build_restore=True)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\n",
            "    build_restore=build_restore)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 502, in _build_internal\n",
            "    restore_sequentially, reshape)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 381, in _AddShardedRestoreOps\n",
            "    name=\"restore_shard\"))\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 328, in _AddRestoreOps\n",
            "    restore_sequentially)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 575, in bulk_restore\n",
            "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n",
            "    name=name)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1296, in restore\n",
            "    names_to_keys = object_graph_key_mapping(save_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1614, in object_graph_key_mapping\n",
            "    object_graph_string = reader.get_tensor(trackable.OBJECT_GRAPH_PROTO_KEY)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 678, in get_tensor\n",
            "    return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt-main\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/opennmt/bin/main.py\", line 186, in main\n",
            "    log_time=args.log_prediction_time)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/opennmt/runner.py\", line 423, in infer\n",
            "    hooks=infer_hooks):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 635, in predict\n",
            "    hooks=all_hooks) as mon_sess:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\n",
            "    stop_grace_period_secs=stop_grace_period_secs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 725, in __init__\n",
            "    self._sess = _RecoverableSession(self._coordinated_creator)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1200, in __init__\n",
            "    _WrappedSession.__init__(self, self._create_session())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1205, in _create_session\n",
            "    return self._sess_creator.create_session()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 871, in create_session\n",
            "    self.tf_sess = self._session_creator.create_session()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 647, in create_session\n",
            "    init_fn=self._scaffold.init_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py\", line 290, in prepare_session\n",
            "    config=config)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py\", line 204, in _restore_checkpoint\n",
            "    saver.restore(sess, checkpoint_filename_with_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 1302, in restore\n",
            "    err, \"a Variable name or other graph key that is missing\")\n",
            "tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n",
            "\n",
            "2 root error(s) found.\n",
            "  (0) Not found: Key transformer/decoder/LayerNorm/beta not found in checkpoint\n",
            "\t [[node save/RestoreV2 (defined at /lib/python3.6/dist-packages/opennmt/runner.py:423) ]]\n",
            "  (1) Not found: Key transformer/decoder/LayerNorm/beta not found in checkpoint\n",
            "\t [[node save/RestoreV2 (defined at /lib/python3.6/dist-packages/opennmt/runner.py:423) ]]\n",
            "\t [[save/RestoreV2/_17]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "Original stack trace for 'save/RestoreV2':\n",
            "  File \"/bin/onmt-main\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/lib/python3.6/dist-packages/opennmt/bin/main.py\", line 186, in main\n",
            "    log_time=args.log_prediction_time)\n",
            "  File \"/lib/python3.6/dist-packages/opennmt/runner.py\", line 423, in infer\n",
            "    hooks=infer_hooks):\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 635, in predict\n",
            "    hooks=all_hooks) as mon_sess:\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1007, in __init__\n",
            "    stop_grace_period_secs=stop_grace_period_secs)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 725, in __init__\n",
            "    self._sess = _RecoverableSession(self._coordinated_creator)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1200, in __init__\n",
            "    _WrappedSession.__init__(self, self._create_session())\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 1205, in _create_session\n",
            "    return self._sess_creator.create_session()\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 871, in create_session\n",
            "    self.tf_sess = self._session_creator.create_session()\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 638, in create_session\n",
            "    self._scaffold.finalize()\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py\", line 229, in finalize\n",
            "    self._saver = training_saver._get_saver_or_default()  # pylint: disable=protected-access\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 599, in _get_saver_or_default\n",
            "    saver = Saver(sharded=True, allow_empty=True)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 825, in __init__\n",
            "    self.build()\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 837, in build\n",
            "    self._build(self._filename, build_save=True, build_restore=True)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 875, in _build\n",
            "    build_restore=build_restore)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 502, in _build_internal\n",
            "    restore_sequentially, reshape)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 381, in _AddShardedRestoreOps\n",
            "    name=\"restore_shard\"))\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 328, in _AddRestoreOps\n",
            "    restore_sequentially)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/training/saver.py\", line 575, in bulk_restore\n",
            "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/ops/gen_io_ops.py\", line 1696, in restore_v2\n",
            "    name=name)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3616, in create_op\n",
            "    op_def=op_def)\n",
            "  File \"/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 2005, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqKc5pTtdZtJ",
        "colab_type": "text"
      },
      "source": [
        "# Translate to your chosen model\n",
        "\n",
        "Add\n",
        "\n",
        "--checkpoint_path toy-ende/run/model.ckpt-YOUR_MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j41hx5d5ddWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!onmt-main infer --auto_config    --config data.yml      --features_file toy-ende/src-test.txt --predictions_file toy-ende/pred.txt --checkpoint_path toy-ende/run/model.ckpt-YOUR_MODEL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eOOdsGNdB6h",
        "colab_type": "text"
      },
      "source": [
        "# Detokenization\n",
        "\n",
        "Even after the translation process is finished, it is still in a segment, so it is different from the actual sentence structure used by real people. Thus, when you perform a detoxification process, it is returned in the form of the actual sentence.\n",
        "\n",
        "We Use \"sed\" for BPE Detokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODjX2TFBdEKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sed -i \"s/@@ //g\"  toy-ende/pred.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yOBYnssd0-g",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Using BLEU\n",
        "\n",
        "Quantitative evaluation is performed on the sentence thus obtained. BLEU is a quantitative evaluation method for machine translation. You can see which model is superior by comparing it to the BLEU score you are comparing.\n",
        "\n",
        "https://www.aclweb.org/anthology/P02-1040"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WmX7oukd2WW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perl  OpenNMT-tf/third_party/multi-bleu.perl toy-ende/ref.txt < toy-ende/pred.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scYq_-MxeGPo",
        "colab_type": "text"
      },
      "source": [
        "If you have Any Question Please Email to  \"bcj1210@naver.com\""
      ]
    }
  ]
}